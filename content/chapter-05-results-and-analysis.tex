\chapter{Results and Analysis}
\label{ch:resultsAndAnalysis}


% \engExpl{Sometimes this is split into two chapters.\\Keep in mind: How you are going to evaluate what you have done? What are your metrics?\\Analysis of your data and proposed solution\\Does this meet the goals which you had when you started?}


This chapter will present and analyse the results of the research conducted in this thesis.


% \sweExpl{I detta kapitel presenterar vi resultaten och diskutera dem.\\Ibland delas detta upp i två kapitel.\\Hur du ska utvärdera vad du har gjort? Vad är din statistik?\\Analys av data och föreslagen lösning\\Innebär detta att uppfyllelse av de mål som du hade när du började?}


% \sweExpl{Huvudsakliga resultat}


\section{Feasibility of building an AI assistant on open source technologies}


One of the goals of the research in this thesis was, as outlined in section~\ref{sec:goals}, to assess the feasibility of building an AI-assistant on open-source technologies and deploying the agent in an academic setting. This section will outline the results and showcase the impact open source tooling had on the implementation of the AI assistant.


\subsection{How popular was the system}


The system was developed during the spring of 2024 and gradually deployed to seven real courses at KTH starting on the 18th of April 2024. The students in the courses that participated in the study held a total of \input{results/latex_variables/usage-01-total-number-of-chats}chats and the users of the system sent \input{results/latex_variables/usage-07-total-number-of-messages}messages. As can be seen in \autoref{fig:usage_01_cumulative_number_of_chats} and \autoref{fig:usage_08_number_of_messages_per_day} these steadily increased over the course of the study as students initiated new chats with the assistant.


\input{results/plots/usage-01-cumulative-number-of-chats}


\input{results/plots/usage-08-number-of-messages-per-day}


Separating the chats initiated in the separate course rooms we observe that some courses followed a fairly linear increase in the number of chats. One example of this is the course \textit{MG2040 Assembly Technology 6.0 credits}, which can be seen in figure~\ref{fig:usage_02_cumulative_number_of_chats_per_course}.


\input{results/plots/usage-02-cumulative-number-of-chats-per-course}


Looking at other courses in figure~\ref{fig:usage_02_cumulative_number_of_chats_per_course} it is evident that not all courses follow the same pattern as \textit{MG2040}. Some courses initially have very few chats due to the chatbot not being deployed simultaneously across all courses. \autoref{tab:course_start_dates} details the start dates for each course. For instance, \textit{DD1349 Projektuppgift i introduktion till datalogi 3,0 hp} exhibits a steep increase in users when it launched, followed by no further growth. This is because the course officially ended shortly after the chatbot was introduced. In all courses participating in the study, the chatbot was deployed well after the courses had already begun, therefore a similar pattern can be seen in many other courses.


\input{content/tables/02-start-dates-for-courses}


\autoref{fig:usage_03_number_of_chats_per_course} shows the total number of chats held in each course, and \autoref{fig:usage_03_number_of_chats_per_calendar_week_per_course} shows how these were distributed over each course over time. The course \textit{MG2040} held the most, \input{results/latex_variables/usage-04-chats-mg2040}chats. \textit{DD1349} held the second most and \textit{DD1367} the third most, \input{results/latex_variables/usage-05-chats-dd1349}and \input{results/latex_variables/usage-06-chats-dd1367}chats respectively.


\input{results/plots/usage-05-number-of-chats-per-course}


\input{results/plots/usage-04-number-of-chats-per-calendar-week-per-course}




Looking at the number of sessions created in \autoref{fig:usage_06_number_of_sessions_per_day}, we can see a similar linear pattern. A session is started whenever a user loads the application without already having loaded it before. A session is not tracked between devices, therefore a user would have two sessions if the same user accessed the chat on two different devices, such as a desktop and a mobile phone. However, the same session is used across courses. Looking at the breakdown per course in \autoref{fig:usage_07_number_of_sessions_per_day_and_course}, we can see that this is likely the case for the course \textit{MG2040 Assembly Technology}.


\input{results/plots/usage-06-number-of-sessions-per-day}


\input{results/plots/usage-07-number-of-sessions-per-day-and-course}


Looking at \textit{MG2040}, it looks like nearly 200\% of its 32 students have started a session. This discrepancy is likely due to the following two reasons,


\begin{enumerate}
        \item The inclusion of teachers and TAs who also use the service. In a small course, this significantly skews the percentages that can be seen in \autoref{fig:usage_07_number_of_sessions_per_day_and_course}.
        \item Sessions are counted uniquely across different devices. This factor likely affects other courses as well, although it is most evident in \textit{MG2040}.
\end{enumerate}




Looking at the distribution of how many chats and messages is sent per session, as seen in figure \autoref{fig:usage_12_number_of_sessions_with_number_of_chats} and \autoref{fig:usage_13_number_of_sessions_with_number_of_messages} we can see that it was very common for users to only start one or two chats. Most users sent quite a few messages though. The average user held \input{results/latex_variables/usage-02-average-chats-per-session}chats and sent \input{results/latex_variables/usage-03-average-messages-per-session}messages.


\input{results/plots/usage-12-number-of-sessions-with-number-of-chats}


\input{results/plots/usage-13-number-of-sessions-with-number-of-messages}


\subsection{Open source v. Proprietary LLMs}


With regards to the feasibility of building AI assistants on open source technologies there are a number of metrics to look at for comparing open source \gls{LLM}s to proprietary language models. \autoref{tab:sessions_chats_and_messages_by_model} shows metrics for both models that were included in the experiment. The experiment was designed to sample between the included models randomly, and as we can see the number of sessions started between the two models are virtually the same. However, there are notable differences in the number of chats started and messages sent between the two. The proprietary model, \textit{GPT-4} by OpenAI, leads the open source model, \textit{Mistral-7B-Instruct-v0.2} by MistralAI. Section~\ref{sec:qualitative_analysis_of_user_responses} and \ref{sec:impact_of_llm_on_user_preferences} will showcase the user preferences with respect to these models, which could explain the discrepancy between two models with regards to simple usage metrics, which is what is shown in \autoref{tab:sessions_chats_and_messages_by_model}.


\input{results/latex_tables/usage-01-sessions-chats-messages-by-model}


Looking at the operational performance of both models included in \autoref{tab:sessions_chats_and_messages_by_model}, there are two notable metrics that were measured in the experiment with respect to operating these models, more specifically metrics that doesn’t measure the quality of their responses (these are covered in \label{sec:impact_of_llm_on_user_preferences}). The metrics are; the response time for the model and time taken to generate queries. The latter is measuring what is generated by the system to query the index that was produced when crawling the course room. The \gls{LLM} is obviously used to generate the assistant's next message, but it is also used to generate a search query, based upon the current conversation between the assistant and the user. The time taken to generate this query is obviously important for the overall performance of the system.


\autoref{fig:performance_05_daily_average_response_time_including_pending_time} shows the daily average response for each model. This includes the time taken before a worker node had picked up the workload. This is important because in the event of high traffic to the system, \gls{LLM} tasks could be queued up and response times could increase. The chart shows that the two models are generally very similar in terms of the time it takes them to produce a reply to the user's question. It is notable however, that the open source alternative (\textit{Mistral-7B-Instruct-v0.2}) has higher peaks on certain days.


\input{results/plots/performance-05-daily-average-response-time-per-model-including-pending-time}


\autoref{fig:performance_06_generating_queries} shows the time taken to produce queries. Similar to what could be said about \autoref{fig:performance_05_daily_average_response_time_including_pending_time}, both models perform very similarly. However, in this metric the open source alternative is faster. The reason the open source model outperforms \textit{GPT-4} on this metric is likely due to the higher latency sometimes observed on the OpenAI API. The custom built solution to operate \gls{LLM} for this study exhibits much lower latency.


\input{results/plots/performance-06-generating-queries}


\subsection{Open source v. Proprietary Embedding functions}


The intention of the research in this thesis was to compare open source embedding functions with proprietary alternatives commonly used in \gls{RAG}-based applications. In addition to this, the experiment was also designed to be able to measure vector search as a retrieval technique, with traditional full text search. However, due to the number of students that participated in the study, no configuration was used that didn’t use the vector search strategy with the embedding function \textit{text-embedding-3-large} by OpenAI \cite{openai_new_2024}. So no metrics were captured in the retrieval phase for any other strategy or model. The retrieval time taken for the model that was used is shown in \autoref{fig:performance_07_embedding_function_query_performance}. Metrics were, however, captured during the indexing phase on other models.


\input{results/plots/performance-07-embedding-function-query-performance}


\subsubsection{Understanding the indexing}
\label{sec:understanding_indexing}


The indexing of course rooms was done regularly. It wasn’t done on a schedule, it was instead done on an ad-hoc basis whenever a course was updated with new content. \autoref{fig:performance_01_timeline_of_snapshots_taken} shows a timeline for each course which includes the date for when each snapshot of the course room was taken.


\input{results/plots/performance-01-timeline-of-snapshots-taken}


Each course room is different and includes pages and content of varying length. No exact metric for how much content was included in each course room is presented in this section, however \autoref{fig:performance_02_urls_per_course} shows how many urls the crawler found in each course, and how many of them were indexed. This is an imperfect, yet decent proxy for how "large" a course room was.


\input{results/plots/performance-02-urls-per-course}


\subsubsection{Measuring indexing performance}
\label{sec:measuring_indexing_performance}


The time taken by indexing course rooms varied widely depending on the load on the system. If more than one course room were indexed simultaneously, indexing took much longer. The most time consuming part of indexing a course room was computing the embeddings for all documents added to the index. \autoref{tab:average_response_time_embedding_functions} shows the open source embedding function used in the experiment, \textit{Salesforce’s} \textit{SFR-Embedding-Mistral}\cite{meng_sfr-embedding-mistral_2024}, which was chosen because it had the highest score on the \gls{MTEB}-benchmark\footnote{\href{https://huggingface.co/spaces/mteb/leaderboard}{Massive Text Embedding Benchmark (MTEB) Leaderboard on Huggingface} (accessed on May 1, 2024)}, is two orders of magnitude slower than \textit{text-embedding-3-large}, the currently best embedding function developed by OpenAI. The reason for this was likely the execution environment chosen for the open source candidate.


\input{results/latex_tables/performance-02-embedding-function-indexing}


During the experiment the embedding functions utilised the same servers that ran the open source \gls{LLM}s. To utilise the hardware rented for this thesis optimally, the open source embedding models were executed on the unutilised CPUs of the servers which ran the \gls{LLM}s on their attached GPU devices. The embedding models are, as opposed to the \gls{LLM}s at least feasible to run using a CPU only. However, as shown in \autoref{tab:average_response_time_embedding_functions}, this had quite a drastic impact on the indexing performance.


Had the open source models been used for retrieval, the performance difference would likely not have been as big. As shown in \autoref{tab:tab:average_response_time_by_length_embedding_functions} the difference in computation time between \textit{GPT-4} and \textit{SFR-Embedding-Mistra} is much smaller for smaller documents. Computing the embedding of a user query, which is what is done during retrieval, equates to computing the embedding for a very small document.


\input{results/latex_tables/performance-01-embedding-function-indexing-prompt-length}




\section{The impact of different LLM models on the speed, accuracy and reliability of responses}
\label{sec:impact_of_llm_on_user_preferences}


This section will present and analyse the gathered data on user preference and technological efficacy of different tools and technologies such as different \gls{RAG} toolchains and \gls{LLM}, as outlined in section~\ref{sec:goals}.


\subsection{Thumbs up/Thumbs down responses to FAQ questions}


After each response to a question that was selected from the frequently asked questions (FAQs) that were shown before a user had sent any messages, as shown in \autoref{fig:faq_questions}, the user was presented with an optional binary thumbs up/down vote on the quality of the response. Both had a tooltip that said \textit{"This was a good response"} and \textit{"This was a bad response"} respectively. \autoref{tab:inserted_questions_faq} shows how many users answered this question. \autoref{fig:feedback_02_frequency_of_answer_for_question_per_model_2e09fd} shows how the participants in the study voted.


\input{content/tables/11-feedback-questions-faq}


\input{results/plots/feedback-02-frequency-of-answer-for-question-per-model-2e09fd}


Generally, it can be observed that users had a positive response to the replies produced to the FAQs. Almost twice as many positive answers were recorded as negative responses.


Looking at the breakdown per model, the open-source alternative \textit{Mistral-7B-Instruct-v0.2}, had almost the same amount of positive and negative responses, which can be seen in \autoref{fig:feedback_02_frequency_of_answer_for_question_per_model_2e09fd}. The proprietary model \textit{GPT-4} had a significantly higher proportion of positive responses, which indicates a generally more favourable reception from the participants in the study.


\subsection{Survey questions injected into the chat}
\label{sec:survey_questions_injected_into_the_chat}


The software written for this thesis was, as discussed in sections \ref{sec:method_feedback_data} and \ref{sec:what_you_did_gathering_feedback_data}, and shown in \autoref{fig:multiple_choice}, designed to gather user feedback by injecting questions in the chat at certain triggers. The questions inserted after receiving the first response in chat number 2, 4 and 6 of each session can be seen in \autoref{tab:inserted_questions_2_4_6}. The questions inserted after receiving the first response in chat number 8 can be seen in \autoref{tab:inserted_questions_2_4_6}.


Unfortunately, few of the users who actually used the system answered these questions. As \autoref{tab:inserted_questions_2_4_6} and \autoref{tab:inserted_questions_2_4_6} shows, many users answer the first set of questions, but very few used the system enough to get the questions inserted into their 8th chat, and those who did use it that much - did not answer it.


\input{content/tables/03-feedback-questions-first-three-times}


\input{content/tables/04-feedback-questions-the-last-time}


Since the number of answers weren't many, the study couldn’t sample between more than two different configurations. The students did not know which configuration they got sampled, nor that there existed different configurations. These two configurations used different \gls{LLM}s, but shared the same retrieval strategy (vector search) and embedding function (\textit{openai/text-embedding-3-large}). In figures \ref{fig:feedback_02_frequency_of_answer_for_question_per_model_cbfea1}, \ref{fig:feedback_02_frequency_of_answer_for_question_per_model_ead094} and \ref{fig:feedback_02_frequency_of_answer_for_question_per_model_d474ac} how users that were assigned the different \gls{LLM}s answered can be seen.


\input{results/plots/feedback-02-frequency-of-answer-for-question-per-model-cbfea1}


\input{results/plots/feedback-02-frequency-of-answer-for-question-per-model-ead094}


\input{results/plots/feedback-02-frequency-of-answer-for-question-per-model-d474ac}


\section{Qualitative analysis of free-text answers}
\label{sec:qualitative_analysis_of_user_responses}


This section will present an analysis of the free text answers users have provided in the forms that have been presented in the participating courses. The complete answers to these forms can be found in \autoref{appendix:mg2040_form}, \autoref{appendix:ld1000_form} and \autoref{appendix:ld1006_form}. With these answers there is no way to correlate the answers with which chat configuration they had used, such as what \gls{LLM} had been used.


\subsection{The form submitted in MG2040}


\subsubsection{Can you describe a situation where the chatbot was particularly helpful or fell short of your expectations?}


The answers to this question show that users found the chatbot useful when asking questions about specific assignments, such as the Assembly project. One student say that it was surprisingly good at helping them with calculations for the project. Other used it to find certain topics and modules in the courseroom and thought the chatbot helped them to find it quickly.


There were also instances where the chatbot wasn’t particularly helpful. One student said it couldn’t provide the date of the exam. Others said its answers were too generic.


\subsubsection{Which type of questions would you ask the chatbot as opposed to the teacher or teaching assistants?}


The students used it to ask questions they deemed unnecessary to ask the course instructors directly. This could be questions about deadlines, number of lectures or other logistical details. Additionally, users mentioned asking the chatbot for detailed explanations with examples and for help with understanding course material. This could reportedly be late at night when course administrators aren’t available.


Some users also prefer the chatbot for questions that might be too minor or unrecognised by teaching assistants. These could be questions that does not require the expertise of a teacher or TA.


\subsubsection{Has using the chatbot changed the way you access information for your courses? If so, how?}


The chatbot has altered the course room experience for some students. While some students reported minimal change, either because of the chatbot's novelty or lack of interest, others found it helped them to access information quicker. For instance, one student reported they asked the chatbot questions instead of searching through course room documents manually. Users also mentioned that the chatbot provided good explanations of content.


\subsubsection{What has been your overall experience using the bot for course-related queries?}


Overall students who tried the chatbot reported positive, and very positive, responses. One student reported it was surprisingly effective. Others reported that it was fun and easy to use. Although some responses are seemingly very generic, the general consensus seems to be that the chatbot has been a beneficial addition to their study resources.


\subsubsection{Why have you not used the chatbot?}


The students who began the form by indicating they had not tried the chatbot got the opportunity to answer the question why they had not tried it. Most users reported that they had not had time to try it, or it was not a priority of theirs to test it. One user expressed a lack of perceived necessity, seemingly satisfied with the current mechanisms of navigating the course material.


One user reported that the reason they had not tried the chatbot was because it was monitored by the teachers of the course. This indicates that some students might feel uneasy using this technology, because they are not sure what they are allowed to ask the chatbot.


\subsection{The form submitted in LD1000}


\subsubsection{What was your experience using the chatbot for course-related questions?}


Users found the chatbot to be helpful. This was especially true for the course-related queries. Some users noted that the chatbot was very detailed in its answers, others thought it provided very high-level answers. One user noted that the bot included references to where more info could be found, which that user thought was very helpful. Some users expressed that they were surprised how well it worked. Overall the responses suggests that the chatbot was generally well-received.


\subsubsection{Were there situations where the chatbot was particularly helpful or did not meet your expectations?}


Most responses recorded a situation they thought were particularly helpful. No user used this question to report an event they thought the bot did not meet their expectations. Users found the chatbot helpful when it provided detailed information, including links to content in the course room. Some users reported that they found it useful to be able to ask follow-up questions, with more tailored questions depending on the answer to their first questions.


\subsubsection{Are there questions you prefer to ask the chatbot instead of a teacher or teaching assistant? Which ones?}


Answers here generally fell into two categories of seemingly similar size. One category reported that they appreciate the ability to ask the bot simpler questions, perhaps about course logistics, when the teacher isn’t online or available. Some users prefer the immediacy and convenience of asking a chatbot. Some still noted though that they generally prefer the human connection of asking a teacher or TA.


The second category was users who reported they don’t prefer asking any questions to a chatbot over a teacher. The reason here was generally that they preferred the "human-connection" of asking a human teacher. Where one suggested there was a trust issue with asking a chatbot over a human.


\subsubsection{Do you think using the chatbot can change how you access information in a course? How?}


The responses to this question generally indicate students think it would mostly change how they access information in courses with larger canvas rooms. That is canvas rooms with lots of pages and difficult to understand navigation. Some reported that they generally would prefer better structured course rooms, but when that’s not possible the chatbot could be a nice complement.


Some users didn’t think the chatbot would change how they accessed information. This was either due to not having used it much in this course, or because they thought they would prefer talking to a human teacher.


\subsubsection{What do you think about using AI tools in your studies? Do you use any tools today? Which ones, and for what?}


A minority of the responses reported that they used AI tools in their academic studies. Some reported that they used it for programming assignments. Others said that they use it for explanations or clarifications in texts they’ve written. Most did report that they don’t use it though. Some reasons for this were that they hadn’t found it useful, preferred to do stuff themselves because it was more creative and it could be considered cheating.


\subsection{The form submitted in LD1006}


\subsubsection{What was your experience of using the chatbot for course-related questions?}


The responses to this question were mixed. Some users seemed surprised at how well it worked, some thought the answers were too vague to be useful. Some users reported that the answers were inaccurate, one user provided an example where the chatbot had said zoom meetings were recorded and published on the canvas page, something they claimed was inaccurate. Many answers were positive, where users claimed they’ve received accurate information.


\subsubsection{Were there any situations where the chatbot was particularly helpful or where it fell short of your expectations?}


Generally responses here fall into two categories. The first category is users who are satisfied with the answers and think the chatbot answered their questions satisfactorily. Some provide extra information of a particular instance where the chatbot answered a specific question. The second category is unsatisfied users. The reason they report they aren’t satisfied varies, some report that the bot failed at answering a specific question. One user reported that the chatbot failed at providing references to the course literature.


There is also a small number of users who can’t recall a specific instance where the bot either exceeded or did not meet their expectations.


\subsubsection{Are there questions you would rather ask the chatbot than a teacher or teaching assistant? Which ones?}


Responses to this question mainly indicate that users can’t think of any given question that they would prefer to ask. Most users simply report that they can’t think of any such questions. Some answers indicate that with more usage they might think of questions they would prefer to ask.


Some users answer more generally that they either weren't exactly sure who to ask about a certain question, and in scenarios like that, the chatbot might be preferable. Similarly, when either the teacher isn’t available or the question is very simple, their answer indicate that they prefer a chatbot


\subsubsection{Do you think the use of the chatbot could change how you access information in a course? How?}


The responses indicate that many users are sceptical of the chatbot having a material impact on how they access course information. Some users report that it might, some are ambiguous. Those who think that it might change how they access course information mention that it might be useful for certain particular tasks, such as summarisation. Others think that if the chatbot’s intelligence generally improves, the bot will change how they access information.


Those who think that it won’t change how they access information, sometimes reference they are new to the technology and aren’t sure how to use it. Most responses that indicate the user doesn’t think that it will change how they access course information, don't elaborate on why.




\subsubsection{How do you think about the use of AI tools by both yourself and your students? Do you and/or your students use any tools today? Which ones, for what?}


This question prompted the longest answers, many responded with very detailed answers indicating this is a question many respondents feel strongly about. The answers indicated a broad range of attitudes towards the use of AI tools in the educational setting. Some users are positive about the potential of AI tools and their ability to enhance student learning. Some respondents included stories where their students had used AI for student-to-student discussions, providing quick information, and aiding in planning and breaking down assignments. Those who are positive generally still stress the importance of educating both students and educators about AI risks, and how to use it effectively. Some mention that AI will be more effective in certain subjects.


The users who expressed caution about using AI within education often had stories in which AI had been used with either little success or very poor outcome. One respondent mentioned students had stopped using it for solving their word problems in their class because its results were simply too bad. Many respondents mentioned ChatGPTs, and other AIs like it, poor abilities in solving maths problems, as hinders to being used in education.


Some responses indicated that the respondent felt generally unprepared to use AI. One respondent admits to not be using AI tools due to their own lack of knowledge. They expressed a need to learn more before incorporating them into teaching their own teaching.


Many respondents raised concerns about authenticity and plagiarism. They want the ability to distinguish between work created by an AI and their student’s own work. Another common theme was the importance of critical thinking, where one response emphasised the need to teach students how to critically read and evaluate AI-generated content to ensure they can understand what a model generates is high quality, and what is lower quality.




% \sweExpl{Lite statistik av fördröjningsmätningarna visas i Tabell~\ref{tab:delayMeasurements}. Förseningen har beräknats från den tidpunkt då begäran GET tas emot fram till svaret skickas.}


% \section{Reliability Analysis}


% \sweExpl{Analys av tillförlitlighet\\
% Tillförlitlighet i metod och data}


% \section{Validity Analysis}


% \sweExpl{Analys av validitet\\
% Validitet i metod och data}


\cleardoublepage