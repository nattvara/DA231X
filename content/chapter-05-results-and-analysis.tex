\chapter{Results and Analysis}
\label{ch:resultsAndAnalysis}


% \engExpl{Sometimes this is split into two chapters.\\Keep in mind: How you are going to evaluate what you have done? What are your metrics?\\Analysis of your data and proposed solution\\Does this meet the goals which you had when you started?}


This chapter will present and analyse the results of the research conducted in this thesis.


% \sweExpl{I detta kapitel presenterar vi resultaten och diskutera dem.\\Ibland delas detta upp i två kapitel.\\Hur du ska utvärdera vad du har gjort? Vad är din statistik?\\Analys av data och föreslagen lösning\\Innebär detta att uppfyllelse av de mål som du hade när du började?}


% \sweExpl{Huvudsakliga resultat}


\section{Feasibility of building an AI assistant on open source technologies}


One of the goals of the research in this thesis was, as outlined in section~\ref{sec:goals}, to assess the feasibility of building an AI-assistant on open-source technologies and deploying the agent in an academic setting. This section will outline the results and showcase the impact open source tooling had on the implementation of the AI assistant.


\subsection{How popular was the system}


The system was developed during the spring of 2024 and gradually deployed to seven real courses at KTH starting on the 18th of April 2024. The students in the courses that participated in the study held a total of \input{results/latex_variables/usage-01-total-number-of-chats}chats and the users of the system sent \input{results/latex_variables/usage-07-total-number-of-messages}messages. As can be seen in \autoref{fig:usage_01_cumulative_number_of_chats} and \autoref{fig:usage_08_number_of_messages_per_day} these steadily increased over the course of the study as students initiated new chats with the assistant.


\input{results/plots/usage-01-cumulative-number-of-chats}


\input{results/plots/usage-08-number-of-messages-per-day}


Separating the chats initiated in the separate course rooms we observe that some courses followed a fairly linear increase in the number of chats. One example of this is the course \textit{MG2040 Assembly Technology 6.0 credits}, which can be seen in figure~\ref{fig:usage_02_cumulative_number_of_chats_per_course}.


\input{results/plots/usage-02-cumulative-number-of-chats-per-course}


Looking at other courses in figure~\ref{fig:usage_02_cumulative_number_of_chats_per_course} it is evident that not all courses follow the same pattern as \textit{MG2040}. Some courses initially have very few chats due to the chatbot not being deployed simultaneously across all courses. \autoref{tab:course_start_dates} details the start dates for each course. For instance, \textit{DD1349 Projektuppgift i introduktion till datalogi 3,0 hp} exhibits a steep increase in users when it launched, followed by no further growth. This is because the course officially ended shortly after the chatbot was introduced. In all courses participating in the study, the chatbot was deployed well after the courses had already begun, therefore a similar pattern can be seen in many other courses.


\input{content/tables/02-start-dates-for-courses}


\autoref{fig:usage_03_number_of_chats_per_course} shows the total number of chats held in each course, and \autoref{fig:usage_03_number_of_chats_per_calendar_week_per_course} shows how these were distributed over each course over time. The course \textit{MG2040} held the most, \input{results/latex_variables/usage-04-chats-mg2040}chats. \textit{DD1349} held the second most and \textit{DD1367} the third most, \input{results/latex_variables/usage-05-chats-dd1349}and \input{results/latex_variables/usage-06-chats-dd1367}chats respectively.


\input{results/plots/usage-05-number-of-chats-per-course}


\input{results/plots/usage-04-number-of-chats-per-calendar-week-per-course}




Looking at the number of sessions created in \autoref{fig:usage_06_number_of_sessions_per_day} and \autoref{fig:usage_07_number_of_sessions_per_day_and_course}, we can see a similar pattern linear pattern. A session is started whenever a user loads the application without already having loaded it before. A session is not tracked between devices, therefore a user would have two sessions if the same user accessed the chat on two different devices, such as a desktop and a mobile phone. However, the same session is used across courses.


\input{results/plots/usage-06-number-of-sessions-per-day}


\input{results/plots/usage-07-number-of-sessions-per-day-and-course}


Looking at the distribution of how many chats and messages is sent per session, as seen in figure \autoref{fig:usage_12_number_of_sessions_with_number_of_chats} and \autoref{fig:usage_13_number_of_sessions_with_number_of_messages} we can see that it was very common for users to only start one or two chats. Most users sent quite a few messages though. The average user held \input{results/latex_variables/usage-02-average-chats-per-session}chats and sent \input{results/latex_variables/usage-03-average-messages-per-session}messages.


\input{results/plots/usage-12-number-of-sessions-with-number-of-chats}


\input{results/plots/usage-13-number-of-sessions-with-number-of-messages}


\subsection{Open source v. Proprietary LLMs}


With regards to the feasibility of building AI assistants on open source technologies there are a number of metrics to look at for comparing open source \gls{LLM}s to proprietary language models. \autoref{tab:sessions_chats_and_messages_by_model} shows metrics for both models that were included in the experiment. The experiment was designed to sample between the included models randomly, and as we can see the number of sessions started between the two models are virtually the same. However, there are notable differences in the number of chats started and messages sent between the two. The proprietary model, \textit{GPT-4} by OpenAI, leads the open source model, \textit{Mistral-7B-Instruct-v0.2} by MistralAI. Section~\ref{sec:qualitative_analysis_of_user_responses} and \ref{sec:impact_of_llm_on_user_preferences} will showcase the user preferences with respect to these models, which could explain the discrepancy between two models with regards to simple usage metrics, which is what is shown in \autoref{tab:sessions_chats_and_messages_by_model}.


\input{results/latex_tables/usage-01-sessions-chats-messages-by-model}


Looking at the operational performance of both models included in \autoref{tab:sessions_chats_and_messages_by_model}, there are two notable metrics that were measured in the experiment with respect to operating these models, more specifically metrics that doesn’t measure the quality of their responses (these are covered in \label{sec:impact_of_llm_on_user_preferences}). The metrics are; the response time for the model and time taken to generate queries. The latter is measuring what is generated by the system to query the index that was produced when crawling the course room. The \gls{LLM} is obviously used to generate the assistant's next message, but it is also used to generate a search query, based upon the current conversation between the assistant and the user. The time taken to generate this query is obviously important for the overall performance of the system.


\autoref{fig:performance_05_daily_average_response_time_including_pending_time} shows the daily average response for each model. This includes the time taken before a worker node had picked up the workload. This is important because in the event of high traffic to the system, \gls{LLM} tasks could be queued up and response times could increase. The chart shows that the two models are generally very similar in terms of the time it takes them to produce a reply to the user's question. It is notable however, that the open source alternative (\textit{Mistral-7B-Instruct-v0.2}) has higher peaks on certain days.


\input{results/plots/performance-05-daily-average-response-time-per-model-including-pending-time}


\autoref{fig:performance_06_generating_queries} shows the time taken to produce queries. Similar to what could be said about \autoref{fig:performance_05_daily_average_response_time_including_pending_time}, both models perform very similarly. However, in this metric the open source alternative is faster. The reason the open source model outperforms \textit{GPT-4} on this metric is likely due to the higher latency sometimes observed on the OpenAI API. The custom built solution to operate \gls{LLM} for this study exhibits much lower latency.


\input{results/plots/performance-06-generating-queries}


\subsection{Open source v. Proprietary Embedding functions}


\section{The impact of different LLM models on the speed, accuracy and reliability of responses}
\label{sec:impact_of_llm_on_user_preferences}


This section will present and analyse the gathered data on user preference and technological efficacy of different tools and technologies such as different \gls{RAG} toolchains and \gls{LLM}, as outlined in section~\ref{sec:goals}.


\section{Qualitative analysis of user responses}
\label{sec:qualitative_analysis_of_user_responses}


This section will present an analysis of the free text answers users have provided in the forms that have been presented in the participating courses.




% \sweExpl{Lite statistik av fördröjningsmätningarna visas i Tabell~\ref{tab:delayMeasurements}. Förseningen har beräknats från den tidpunkt då begäran GET tas emot fram till svaret skickas.}


\section{Reliability Analysis}


% \sweExpl{Analys av tillförlitlighet\\
% Tillförlitlighet i metod och data}


\section{Validity Analysis}


% \sweExpl{Analys av validitet\\
% Validitet i metod och data}


\cleardoublepage