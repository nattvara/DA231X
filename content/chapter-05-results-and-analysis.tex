\chapter{Results and Analysis}
\label{ch:resultsAndAnalysis}


% \engExpl{Sometimes this is split into two chapters.\\Keep in mind: How you are going to evaluate what you have done? What are your metrics?\\Analysis of your data and proposed solution\\Does this meet the goals which you had when you started?}


This chapter will present and analyse the results of the research conducted in this thesis.


% \sweExpl{I detta kapitel presenterar vi resultaten och diskutera dem.\\Ibland delas detta upp i två kapitel.\\Hur du ska utvärdera vad du har gjort? Vad är din statistik?\\Analys av data och föreslagen lösning\\Innebär detta att uppfyllelse av de mål som du hade när du började?}


% \sweExpl{Huvudsakliga resultat}


\section{Feasibility of building an AI assistant on open source technologies}


One of the goals of the research in this thesis was, as outlined in section~\ref{sec:goals}, to assess the feasibility of building an AI-assistant on open-source technologies and deploying the agent in an academic setting. This section will outline the results and showcase the impact open source tooling had on the implementation of the AI assistant.


\subsection{How popular was the system}


The system was developed during the spring of 2024 and gradually deployed to seven real courses at KTH starting on the 18th of April 2024. The students in the courses that participated in the study held a total of \input{results/latex_variables/usage-01-total-number-of-chats}chats and the users of the system sent \input{results/latex_variables/usage-07-total-number-of-messages}messages. As can be seen in \autoref{fig:usage_01_cumulative_number_of_chats} and \autoref{fig:usage_08_number_of_messages_per_day} these steadily increased over the course of the study as students initiated new chats with the assistant.


\input{results/plots/usage-01-cumulative-number-of-chats}


\input{results/plots/usage-08-number-of-messages-per-day}


Separating the chats initiated in the separate course rooms we observe that some courses followed a fairly linear increase in the number of chats. One example of this is the course \textit{MG2040 Assembly Technology 6.0 credits}, which can be seen in figure~\ref{fig:usage_02_cumulative_number_of_chats_per_course}.


\input{results/plots/usage-02-cumulative-number-of-chats-per-course}


Looking at other courses in figure~\ref{fig:usage_02_cumulative_number_of_chats_per_course} it is evident that not all courses follow the same pattern as \textit{MG2040}. Some courses initially have very few chats due to the chatbot not being deployed simultaneously across all courses. \autoref{tab:course_start_dates} details the start dates for each course. For instance, \textit{DD1349 Projektuppgift i introduktion till datalogi 3,0 hp} exhibits a steep increase in users when it launched, followed by no further growth. This is because the course officially ended shortly after the chatbot was introduced. In all courses participating in the study, the chatbot was deployed well after the courses had already begun, therefore a similar pattern can be seen in many other courses.


\input{content/tables/02-start-dates-for-courses}


\autoref{fig:usage_03_number_of_chats_per_course} shows the total number of chats held in each course, and \autoref{fig:usage_03_number_of_chats_per_calendar_week_per_course} shows how these were distributed over each course over time. The course \textit{MG2040} held the most, \input{results/latex_variables/usage-04-chats-mg2040}chats. \textit{DD1349} held the second most and \textit{DD1367} the third most, \input{results/latex_variables/usage-05-chats-dd1349}and \input{results/latex_variables/usage-06-chats-dd1367}chats respectively.


\input{results/plots/usage-05-number-of-chats-per-course}


\input{results/plots/usage-04-number-of-chats-per-calendar-week-per-course}




Looking at the number of sessions created in \autoref{fig:usage_06_number_of_sessions_per_day} and \autoref{fig:usage_07_number_of_sessions_per_day_and_course}, we can see a similar pattern linear pattern. A session is started whenever a user loads the application without already having loaded it before. A session is not tracked between devices, therefore a user would have two sessions if the same user accessed the chat on two different devices, such as a desktop and a mobile phone. However, the same session is used across courses.


\input{results/plots/usage-06-number-of-sessions-per-day}


\input{results/plots/usage-07-number-of-sessions-per-day-and-course}


Looking at the distribution of how many chats and messages is sent per session, as seen in figure \autoref{fig:usage_12_number_of_sessions_with_number_of_chats} and \autoref{fig:usage_13_number_of_sessions_with_number_of_messages} we can see that it was very common for users to only start one or two chats. Most users sent quite a few messages though. The average user held \input{results/latex_variables/usage-02-average-chats-per-session}chats and sent \input{results/latex_variables/usage-03-average-messages-per-session}messages.


\input{results/plots/usage-12-number-of-sessions-with-number-of-chats}


\input{results/plots/usage-13-number-of-sessions-with-number-of-messages}


\subsection{Open source v. Proprietary LLMs}


With regards to the feasibility of building AI assistants on open source technologies there are a number of metrics to look at for comparing open source \gls{LLM}s to proprietary language models. \autoref{tab:sessions_chats_and_messages_by_model} shows metrics for both models that were included in the experiment. The experiment was designed to sample between the included models randomly, and as we can see the number of sessions started between the two models are virtually the same. However, there are notable differences in the number of chats started and messages sent between the two. The proprietary model, \textit{GPT-4} by OpenAI, leads the open source model, \textit{Mistral-7B-Instruct-v0.2} by MistralAI. Section~\ref{sec:qualitative_analysis_of_user_responses} and \ref{sec:impact_of_llm_on_user_preferences} will showcase the user preferences with respect to these models, which could explain the discrepancy between two models with regards to simple usage metrics, which is what is shown in \autoref{tab:sessions_chats_and_messages_by_model}.


\input{results/latex_tables/usage-01-sessions-chats-messages-by-model}


Looking at the operational performance of both models included in \autoref{tab:sessions_chats_and_messages_by_model}, there are two notable metrics that were measured in the experiment with respect to operating these models, more specifically metrics that doesn’t measure the quality of their responses (these are covered in \label{sec:impact_of_llm_on_user_preferences}). The metrics are; the response time for the model and time taken to generate queries. The latter is measuring what is generated by the system to query the index that was produced when crawling the course room. The \gls{LLM} is obviously used to generate the assistant's next message, but it is also used to generate a search query, based upon the current conversation between the assistant and the user. The time taken to generate this query is obviously important for the overall performance of the system.


\autoref{fig:performance_05_daily_average_response_time_including_pending_time} shows the daily average response for each model. This includes the time taken before a worker node had picked up the workload. This is important because in the event of high traffic to the system, \gls{LLM} tasks could be queued up and response times could increase. The chart shows that the two models are generally very similar in terms of the time it takes them to produce a reply to the user's question. It is notable however, that the open source alternative (\textit{Mistral-7B-Instruct-v0.2}) has higher peaks on certain days.


\input{results/plots/performance-05-daily-average-response-time-per-model-including-pending-time}


\autoref{fig:performance_06_generating_queries} shows the time taken to produce queries. Similar to what could be said about \autoref{fig:performance_05_daily_average_response_time_including_pending_time}, both models perform very similarly. However, in this metric the open source alternative is faster. The reason the open source model outperforms \textit{GPT-4} on this metric is likely due to the higher latency sometimes observed on the OpenAI API. The custom built solution to operate \gls{LLM} for this study exhibits much lower latency.


\input{results/plots/performance-06-generating-queries}


\subsection{Open source v. Proprietary Embedding functions}


The intention of the research in this thesis was to compare open source embedding functions with proprietary alternatives commonly used in \gls{RAG}-based applications. In addition to this, the experiment was also designed to be able to measure vector search as a retrieval technique, with traditional full text search. However, due to the number of students that participated in the study, no configuration was used that didn’t use the vector search strategy with the embedding function \textit{text-embedding-3-large} by OpenAI \cite{openai_new_2024}. So no metrics were captured in the retrieval phase for any other strategy or model. The retrieval time taken for the model that was used is shown in \autoref{fig:performance_07_embedding_function_query_performance}. Metrics were, however, captured during the indexing phase on other models.


\input{results/plots/performance-07-embedding-function-query-performance}


\subsubsection{Understanding the indexing}
\label{sec:understanding_indexing}


The indexing of course rooms was done regularly. It wasn’t done on a schedule, it was instead done on an ad-hoc basis whenever a course was updated with new content. \autoref{fig:performance_01_timeline_of_snapshots_taken} shows a timeline for each course which includes the date for when each snapshot of the course room was taken.


\input{results/plots/performance-01-timeline-of-snapshots-taken}


Each course room is different and includes pages and content of varying length. No exact metric for how much content was included in each course room is presented in this section, however \autoref{fig:performance_02_urls_per_course} shows how many urls the crawler found in each course, and how many of them were indexed. This is an imperfect, yet decent proxy for how "large" a course room was.


\input{results/plots/performance-02-urls-per-course}


To understand why the size of the course room is relevant we need to look at \autoref{fig:performance_03_index_time}, that shows how long each course room took to index. We can see that indexing time for the same course room vary a lot. The reason for this is not that the size of the course room varies over time. Looking at \autoref{fig:performance_03_index_time} we can see that the higher values occur when snapshots are taken simultaneously. The way indexing time is measured is by taking the time between the first url in a course room being crawled, and the last time a url was indexed in that snapshot. \autoref{fig:performance_03_index_time} suggests that there is an operation that takes a lot of time, and the indexer gets overloaded when a lot of courses are being indexed at the same time. Section~\ref{sec:measuring_indexing_performance} will show that this is due to the performance of the open source embedding function used in the experiments.


\input{results/plots/performance-03-index-time}


\subsubsection{Measuring indexing performance}
\label{sec:measuring_indexing_performance}


As shown and explained in section~\ref{sec:understanding_indexing} the most time consuming part of indexing a course room was computing the embeddings for all documents added to the index. \autoref{tab:average_response_time_embedding_functions} shows the open source embedding function used in the experiment, Salesforces’ \textit{SFR-Embedding-Mistral}\cite{meng_sfr-embedding-mistral_2024}, which was chosen because it had the highest score on the \gls{MTEB}-benchmark \footnote{As of \today}, is two orders of magnitude slower than \textit{text-embedding-3-large}, the currently best embedding function developed by OpenAI. The reason for this was likely the execution environment chosen for the open source candidate.


\input{results/latex_tables/performance-02-embedding-function-indexing}


During the experiment the embedding functions utilised the same servers that ran the open source \gls{LLM}s. To utilise the hardware rented for this thesis optimally, the open source embedding models were executed on the unutilised CPUs of the servers which ran the \gls{LLM}s on their attached GPU devices. The embedding models are, as opposed to the \gls{LLM}s at least feasible to run using a CPU only. However, as shown in \autoref{tab:average_response_time_embedding_functions}, this had quite a drastic impact on the indexing performance.


Had the open source models been used for retrieval, the performance difference would likely not have been as big. As shown in \autoref{tab:tab:average_response_time_by_length_embedding_functions} the difference in computation time between \textit{GPT-4} and \textit{SFR-Embedding-Mistra} is much smaller for smaller documents. Computing the embedding of a user query, which is what is done during retrieval, equates to computing the embedding for a very small document.


\input{results/latex_tables/performance-01-embedding-function-indexing-prompt-length}




\section{The impact of different LLM models on the speed, accuracy and reliability of responses}
\label{sec:impact_of_llm_on_user_preferences}


This section will present and analyse the gathered data on user preference and technological efficacy of different tools and technologies such as different \gls{RAG} toolchains and \gls{LLM}, as outlined in section~\ref{sec:goals}.


\subsection{Thumbs up/Thumbs down responses to FAQ questions}


After each response to a question that was selected from the frequently asked questions (FAQs) that were shown before a user had sent any messages, as shown in \autoref{fig:faq_questions}, the user was presented with an optional binary thumbs up/down vote on the quality of the response. Both had a tooltip that said \textit{"This was a good response"} and \textit{"This was a bad response"} respectively. \autoref{fig:feedback_01_frequency_of_answer_for_question_2e09fd} shows how the participants in the study voted. Generally, it can be observed that users had a positive response to the replies produced to the FAQs. Almost twice as many positive answers were recorded as negative responses.


Looking at the breakdown per model, the open-source alternative \textit{Mistral-7B-Instruct-v0.2}, had almost the same amount of positive and negative responses, which can be seen in \autoref{fig:feedback_02_frequency_of_answer_for_question_per_model_2e09fd}. The proprietary model \textit{GPT-4} had a significantly higher proportion of positive responses, which indicates a generally more favourable reception from the participants in the study.


\input{results/plots/feedback-01-frequency-of-answer-for-question-2e09fd}


\input{results/plots/feedback-02-frequency-of-answer-for-question-per-model-2e09fd}


\subsection{Survey questions injected into the chat}


The software written for this thesis was, as discussed in sections \ref{sec:method_feedback_data} and \ref{sec:what_you_did_gathering_feedback_data}, and shown in \autoref{fig:multiple_choice}, designed to gather user feedback by injecting questions in the chat at certain triggers. The questions inserted after receiving the first response in chat number 2, 4 and 6 of each session can be seen in \autoref{tab:inserted_questions_2_4_6}. The questions inserted after receiving the first response in chat number 8 can be seen in \autoref{tab:inserted_questions_2_4_6}.


Unfortunately, very few of the users who used the system answered these questions. \autoref{tab:number_of_answers_received} shows how many answers the first set of questions got, and how many answers the second set of questions got.


However, for the users that did answer the first set of questions, their answers can be seen in figures \ref{fig:feedback_01_frequency_of_answer_for_question_cbfea1}, \ref{fig:feedback_01_frequency_of_answer_for_question_ead094} and \ref{fig:feedback_01_frequency_of_answer_for_question_d474ac}.


\input{content/tables/03-feedback-questions-first-three-times}


\input{content/tables/04-feedback-questions-the-last-time}


\input{content/tables/05-feedback-questions-number-of-answers}


\input{results/plots/feedback-01-frequency-of-answer-for-question-cbfea1}


\input{results/plots/feedback-01-frequency-of-answer-for-question-ead094}


\input{results/plots/feedback-01-frequency-of-answer-for-question-d474ac}


Since the number of answers weren't many, the study couldn’t sample between more than two different configurations. These two configurations used different \gls{LLM}s, but shared the same retrieval strategy (vector search) and embedding function (\textit{openai/text-embedding-3-large}). In figures \ref{fig:feedback_02_frequency_of_answer_for_question_per_model_cbfea1}, \ref{fig:feedback_02_frequency_of_answer_for_question_per_model_ead094} and \ref{fig:feedback_02_frequency_of_answer_for_question_per_model_d474ac} how users that were assigned the different \gls{LLM}s answered can be seen.


\input{results/plots/feedback-02-frequency-of-answer-for-question-per-model-cbfea1}


\input{results/plots/feedback-02-frequency-of-answer-for-question-per-model-ead094}


\input{results/plots/feedback-02-frequency-of-answer-for-question-per-model-d474ac}


\section{Qualitative analysis of free-text answers}
\label{sec:qualitative_analysis_of_user_responses}


This section will present an analysis of the free text answers users have provided in the forms that have been presented in the participating courses. The complete answers to these forms can be found in \autoref{appendix:mg2040_form}, \autoref{appendix:ld1000_form} and \autoref{appendix:ld1006_form}. With these answers there is no way to correlate the answers with which chat configuration they had used, such as what \gls{LLM} had been used.


\subsection{The form submitted in MG2040}


\subsubsection{Can you describe a situation where the chatbot was particularly helpful or fell short of your expectations?}


The answers to this question show that users found the chatbot useful when asking questions about specific assignments, such as the Assembly project. One student say that it was surprisingly good at helping them with calculations for the project. Other used it to find certain topics and modules in the courseroom and thought the chatbot helped them to find it quickly.


There were also instances where the chatbot wasn’t particularly helpful. One student said it couldn’t provide the date of the exam. Others said its answers were too generic.


\subsubsection{Which type of questions would you ask the chatbot as opposed to the teacher or teaching assistants?}


The students used it to ask questions they deemed unnecessary to ask the course instructors directly. This could be questions about deadlines, number of lectures or other logistical details. Additionally, users mentioned asking the chatbot for detailed explanations with examples and for help with understanding course material. This could reportedly be late at night when course administrators aren’t available.


Some users also prefer the chatbot for questions that might be too minor or unrecognised by teaching assistants. These could be questions that does not require the expertise of a teacher or TA.


\subsubsection{Has using the chatbot changed the way you access information for your courses? If so, how?}


The chatbot has altered the course room experience for some students. While some students reported minimal change, either because of the chatbot's novelty or lack of interest, others found it helped them to access information quicker. For instance, one student reported they asked the chatbot questions instead of searching through course room documents manually. Users also mentioned that the chatbot provided good explanations of content.


\subsubsection{What has been your overall experience using the bot for course-related queries?}


Overall students who tried the chatbot reported positive, and very positive, responses. One student reported it was surprisingly effective. Others reported that it was fun and easy to use. Although some responses are seemingly very generic, the general consensus seems to be that the chatbot has been a beneficial addition to their study resources.


\subsubsection{Why have you not used the chatbot?}


The students who began the form by indicating they had not tried the chatbot got the opportunity to answer the question why they had not tried it. Most users reported that they had not had time to try it, or it was not a priority of theirs to test it. One user expressed a lack of perceived necessity, seemingly satisfied with the current mechanisms of navigating the course material.


One user reported that the reason they had not tried the chatbot was because it was monitored by the teachers of the course. This indicates that some students might feel uneasy using this technology, because they are not sure what they are allowed to ask the chatbot.


\subsection{The form submitted in LD1000}


\subsubsection{What was your experience using the chatbot for course-related questions?}


Users found the chatbot to be helpful. This was especially true for the course-related queries. Some users noted that the chatbot was very detailed in its answers, others thought it provided very high-level answers. One user noted that the bot included references to where more info could be found, which that user thought was very helpful. Some users expressed that they were surprised how well it worked. Overall the responses suggests that the chatbot was generally well-received.


\subsubsection{Were there situations where the chatbot was particularly helpful or did not meet your expectations?}


Most responses recorded a situation they thought were particularly helpful. No user used this question to report an event they thought the bot did not meet their expectations. Users found the chatbot helpful when it provided detailed information, including links to content in the course room. Some users reported that they found it useful to be able to ask follow-up questions, with more tailored questions depending on the answer to their first questions.


\subsubsection{Are there questions you prefer to ask the chatbot instead of a teacher or teaching assistant? Which ones?}


Answers here generally fell into two categories of seemingly similar size. One category reported that they appreciate the ability to ask the bot simpler questions, perhaps about course logistics, when the teacher isn’t online or available. Some users prefer the immediacy and convenience of asking a chatbot. Some still noted though that they generally prefer the human connection of asking a teacher or TA.


The second category was users who reported they don’t prefer asking any questions to a chatbot over a teacher. The reason here was generally that they preferred the "human-connection" of asking a human teacher. Where one suggested there was a trust issue with asking a chatbot over a human.


\subsubsection{Do you think using the chatbot can change how you access information in a course? How?}


The responses to this question generally indicate students think it would mostly change how they access information in courses with larger canvas rooms. That is canvas rooms with lots of pages and difficult to understand navigation. Some reported that they generally would prefer better structured course rooms, but when that’s not possible the chatbot could be a nice complement.


Some users didn’t think the chatbot would change how they accessed information. This was either due to not having used it much in this course, or because they thought they would prefer talking to a human teacher.


\subsubsection{What do you think about using AI tools in your studies? Do you use any tools today? Which ones, and for what?}


A minority of the responses reported that they used AI tools in their academic studies. Some reported that they used it for programming assignments. Others said that they use it for explanations or clarifications in texts they’ve written. Most did report that they don’t use it though. Some reasons for this were that they hadn’t found it useful, preferred to do stuff themselves because it was more creative and it could be considered cheating.


\subsection{The form submitted in LD1006}


\subsubsection{Can you describe a situation where the chatbot was particularly helpful or fell short of your expectations?}


TODO


\subsubsection{Which type of questions would you ask the chatbot as opposed to the teacher or teaching assistants?}


TODO


\subsubsection{Has using the chatbot changed the way you access information for your courses? If so, how?}


TODO


\subsubsection{What has been your overall experience using the bot for course-related queries?}


TODO


\subsubsection{Why have you not used the chatbot?}


TODO


% \sweExpl{Lite statistik av fördröjningsmätningarna visas i Tabell~\ref{tab:delayMeasurements}. Förseningen har beräknats från den tidpunkt då begäran GET tas emot fram till svaret skickas.}


% \section{Reliability Analysis}


% \sweExpl{Analys av tillförlitlighet\\
% Tillförlitlighet i metod och data}


% \section{Validity Analysis}


% \sweExpl{Analys av validitet\\
% Validitet i metod och data}


\cleardoublepage