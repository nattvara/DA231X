\chapter{Conclusions and Future work}
\label{ch:conclusionsAndFutureWork}


% \sweExpl{Slutsats och framtida arbete}
% \generalExpl{Add text to introduce the subsections of this chapter.}


This chapter will provide conclusions from the research carried out in this thesis. It will reflect on the goals laid out in section~\ref{sec:goals} and what insights have been gained. It will describe the limitations to the result and discuss some future work.


\section{Conclusions}
\label{sec:conclusions}


% state the goals and research questions


One of the goals for this thesis was to understand the technological efficacy, i.e. speed and accuracy of various different tools and techniques commonly used when developing AI assistants. In addition to that, this thesis tried to understand what users preferred, various tools and the feasibility of operating \gls{LLM}on-premises. Lastly, a goal was to understand how AI assistants impacted education. All of this was encapsulated by the research questions, \textit{Which language model and which retrieval techniques do students prefer using?} and \textit{Is it possible to deploy an AI-assistant using a completely open source toolchain?}. My hypothesis was that the closed source alternatives would be better, however it would be feasible to build a completely open source and self-hosted AI assistant too.


% what was measured closed source models v open sourced


Generally, the hypothesis was correct. The results laid out in \autoref{ch:resultsAndAnalysis}, and section~\ref{sec:survey_questions_injected_into_the_chat} specifically, showed that students preferred the model provided by OpenAI to the Mistral model. Even though that comparison isn’t very fair model-to-model, the fact that self-hosting models that are as large as those available by for profit vendors, means that building an assistant on proprietary models will yield a better assistant. However, the results show generally favourable opinions from the users who had to use the open source model.


% embedding functions and full text search


Due to the size of the experiment, there weren't enough participants to test all the tools and techniques initially intended. The software constructed for the experiments was designed for testing more tools and models, such as some open source embedding functions. However, given these weren’t used in real chats, no conclusions were drawn regarding their effectiveness or effect on student satisfaction. For the same reason, no data was collected on how traditional search techniques, such as fulltext search, affected the metrics collected in the study.


% speed, accuracy and usefulness


The perceived speed, as reported by the users, was just slightly slower for the open source model used, as can be seen in \autoref{fig:feedback_01_frequency_of_answer_for_question_cbfea1}. This was also backed up by the recorded time each response took in \autoref{fig:performance_05_daily_average_response_time_including_pending_time}. The only measurement of accuracy in this study was the perceived accuracy by the users. This is a vert brute metric, however, it did produce a clear result that users perceived \textit{GPT-4} to produce more accurate results than the Mistrals model. Again, this was fairly inline with expectations. Larger open source models might produce even more accurate results. However, a model as capable as \textit{Mistral-7B-Instruct-v0.2}, is not capable enough of building an AI assistant that users overwhelmingly think produce accurate results. The same conclusion can be drawn for the perceived usefulness of the assistants built with the different models.


% understand impact on education


Finally, the research conducted in this thesis did produce some insights on what impact chatbots and AI assistants could have on education. Students appreciated the quick and easy access to information. AI assistants could definitely improve student-teacher communications. This doesn’t mean that all student inquiries should be answered by a chatbot. Students prefer speaking to their actual teacher, however, a chatbot is a useful complement. While implementing such chatbots, the research in this thesis shows the technology selection is important. It is very important the information the bot has access to is accurate. While privacy is paramount for students, it’s still crucial teachers has insight into what the chatbot is telling students.


% re-iterate research question
So in conclusion, the first research question that asked if it was feasible to build and deploy an AI-assistant using a completely open source toolchain can be considered answered. It was feasible, and was done in this thesis. Also, the latter that asked which language models and tools users of the chatbot would prefer was partially answered. The experiments showed conclusive answers in terms of what language models users preferred, other tools and techniques weren't evaluated due to the limited size of participants in the study.


% \sweExpl{Slutsatser}
% \engExpl{Describe the conclusions (reflect on the whole introduction given in Chapter 1).}




% \engExpl{Discuss the positive effects and the drawbacks.\\
% Describe the evaluation of the results of the degree project.\\
% Did you meet your goals?\\
% What insights have you gained?\\
% What suggestions can you give to others working in this area?\\
% If you had it to do again, what would you have done differently?}


% \sweExpl{Uppfyllde du dina mål?\\
% Vilka insikter har du fått?\\
% Vilka förslag kan du ge till andra som arbetar inom detta område?
% Om du skulle göra detta igen, vad skulle du ha gjort annorlunda?}








\section{Limitations}
\label{sec:conclusion_limitations}


The experiments carried out in this thesis were partially able to answer the research questions laid out in \autoref{ch:introduction}. The study would have benefited from more courses that were included in the study. The effort of soliciting courses to join the study was only started in late march, early april. This meant many courses in P4 had already started. This limited the number of courses that were interesting in joining the study, many hesitated changing the format of already started courses. In addition, the courses that did join the study had entered their final half, or final weeks in some cases. This meant that the students probably had already internalised much of the course logistics. Similar studies would with benefit be planned further in advance, ensuring that the experiment is launched just before or in the beginning of a new course. Courses could even be designed around using the chatbot.


Designing studies like the one in this thesis, where large custom software is designed and implemented, would benefit from a more incremental approach. The process for the research in this thesis involved very time consuming software development in the beginning, only to be deployed with too little time to test everything that was included in the software. Therefore, a better approach would have been getting a working assistant, with less features deployed faster. Then iteratively, as there is time left in the study, work on more features. For instance, the experiments never had time to run with post processing of documents enabled, which was a feature that took several weeks to design, implement and test.




% \sweExpl{Begränsande faktorer\\Vad gjorde du som begränsade dina ansträngningar? Vilka är begränsningarna i dina resultat?}
% \engExpl{What did you find that limited your efforts? What are the limitations of your results?}


\section{Future work}
\label{sec:futureWork}






% \sweExpl{Vad du har kvar ogjort?\\Vad är nästa självklara saker som ska göras?\\Vad tips kan du ge till nästa person som kommer att följa upp på ditt arbete?}
% \engExpl{Describe valid future work that you or someone else could or should do.\\
% Consider: What you have left undone? What are the next obvious things to be done? What hints can you give to the next person who is going to follow up on your work?}


There is some obvious future work that was left undone within this thesis. For instance, running the same experiments with different tools and models. This includes testing open source embedding functions as a retrieval technique, and evaluating those similarly to what was done for \gls{LLMs} in this thesis, both against proprietary embedding functions and other retrieval techniques such as full text search and \gls{TF-IDF}.


In addition to this, exploring how chatbots can be granted access to student specific content. Such as their personalised schedule, past assignments and other courses. There are a myriad of other logistical courses students might want to get help with. These include what courses they are eligible for, what they are required to take etc. In all of these cases a chatbot might be a useful tool to deploy. Therefore, future research should be done to investigate how assistants can be given access to this data safely, whilst maintaining user trust as discussed in section~\ref{sec:discussion_privacy}.


\section{Reflections on sustainability}
\label{sec:reflections}


% \sweExpl{Reflektioner}
% \sweExpl{Vilka är de relevanta ekonomiska, sociala, miljömässiga och etiska aspekter av ditt arbete?}
% \engExpl{What are the relevant economic, social,
%   environmental, and ethical aspects of your work?
% }


AI is not a new field of research, and it has been integrated into products and services for years. However, in the last few years their immediate usefulness has become more apparent, especially with the release of tools such as ChatGPT. It is clear the new \gls{LLM}s are very capable and useful. The technology is still very nascent. In the coming years and decades integrating these models into real world applications and business processes will be a long process.


Training machine learning models has traditionally been very compute-intensive, often requiring accelerated compute devices such as GPUs. Computation requires electricity, which depending on the electricity source, means an increased environmental footprint. By extension this means compute intensive applications, such as training models, have an impact on the environment. What’s new with the development of \gls{LLM}s, is that inference, the activity of merely running the models, is also very compute intensive.


The software constructed in this study supported adding server nodes that could run a \gls{LLM} on an attached GPU. During most of the study however, only one server node was used, since the number of students rarely peaked above more than one or two at a time. If the chatbot were to be enabled in more course rooms however, the demands on the available amount of compute-capacity would increase.


The fact that operating applications built on-top of \gls{LLM}s require large amounts of compute capacity, leads to interesting discussions around the \gls{SDG} goals laid out by the United Nations. In the case of the research in this thesis, \gls{SDG} numbers four and thirteen are the most relevant. Goal number four tries to ensure high quality education, while goal thirteen tries to combat climate change and its effects. The research in this thesis has shown that AI-assistants can increase the quality of education and improve the student learning experience. However, since this requires running \gls{LLM}, which has an environmental impact, this results in a trade-off between education and environmental impact.


It’s not clear that \gls{LLM} inference will always have a high environmental impact. New inference technologies such as Groq \cite{abts_software-defined_2022} might result in faster inference with lower energy usage. Achieving energy efficiency in inference would essentially eliminate the tradeoff between the two \gls{SDG} goals, although this remains to be proven.


Operating a \gls{LLM} on-premise isn't always necessary though. However, running the language model yourself offers major privacy benefits. In the case of the application built in this thesis, KTH may need to keep all student-data on their own servers, which could also be the case in other applications in completely different domains, either due to legal or contractual obligations. Besides privacy, performance can also justify on-premise deployments. Having dedicated models available ensures that the performance of applications relying on the \gls{LLM} isn't impacted by the load on third party vendors, such as OpenAI.


By running the model on-premise, this has a higher likelihood of underutilised hardware. Using energy whilst not executing prompts, keeping a model in memory whilst being idle. Using a third party vendor, such as OpenAI or Groq, leads to the pooling of resources between AI applications. This could lead to a higher utilisation of the existing hardware, and reduce the environmental impact. What becomes the de facto standard when building \gls{LLM} application in the future, remains to be seen.


\noindent\rule{\textwidth}{0.4mm}


% \engExpl{In the references, let Zotero or other tool fill this in for you. I suggest an extended version of the IEEE style, to include URLs, DOIs, ISBNs, etc., to make it easier for your reader to find them. This will make life easier for your opponents and examiner. \\IEEE Editorial Style Manual: \url{https://www.ieee.org/content/dam/ieee-org/ieee/web/org/conferences/style_references_manual.pdf}}
% \sweExpl{Låt Zotero eller annat verktyg fylla i det här för dig. Jag föreslår en utökad version av IEEE stil - att inkludera webbadresser, DOI, ISBN osv. - för att göra det lättare för läsaren att hitta dem. Detta kommer att göra livet lättare för dina opponenter och examinator.}


\cleardoublepage