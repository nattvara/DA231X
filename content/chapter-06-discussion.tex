\chapter{Discussion}
\label{ch:discussion}


This chapter will discuss the key findings from the research carried out in this thesis. It will evaluate the implications of the findings and compare it to existing literature. The discussion is structured to provide analysis of the results. It will also explore why some questions laid out in the introduction section have been left unanswered. Finally the chapter will include a brief discussion about the role of AI assistants in the broader context of AI in the educational setting.




\section{Feasibility of Open Source Technologies}


The study, and the software constructed to execute its experiments, show that it is feasible to build and deploy an AI assistant using open source technologies. The most popular models to implement AI assistants are commonly provided under proprietary licences and hidden behind pay-per-use APIs. Examples of this are the models built and sold by OpenAI.


The results laid out in \autoref{ch:whatYouDid} and \autoref{ch:resultsAndAnalysis} shows that an open source alternative is feasible from an implementation standpoint. The open source \gls{LLM} and embedding functions used in the study proved functional and provided comparable performance to proprietary solutions with respect to operational metrics such as response time and query generation time. One learning from operating the software constructed for the study is that the embedding function should probably be operated with a dedicated GPU to improve performance when computing embeddings of larger docs, such as during indexing.


\section{User Engagement and Satisfaction}


The study showed steady engagement from students across multiple different courses. The courses were all fairly different from each other. Some courses were self-study courses, without any lectures, some had regular lectures. Some courses had many participating students, some courses had as little as 30 students in the course.


There was a clear indication that the proprietary models such as OpenAI’s GPT-4 received more favourable feedback compared to its open source counterparts. Unfortunately, due to the volume of participating courses and the number of chats that were held, the decision was made to not test other variables, such as different \gls{RAG} techniques, or embedding functions.


\section{Methodological Improvements}


The software designed for the study aimed to test various tools and techniques commonly used when constructing AI assistants, in alignment with the goals outlined in \ref{sec:goals}. However, shortly after the study's launch, it became evident that the number of participating courses - and consequently, the number of participating students - was insufficient to provide statistically significant results for more than one feature.


The number of participating courses was a function of when the custom software, developed for the study, was finished. Several course administrators and teachers at KTH were contacted and showed interest in participating in the study. The most common reason for not participating was that the course did not run during the study period, i.e. the course responsible had courses in P1, P2 or P3, not P4. The second most common reason was that the teacher held courses that had already started, and did not want to make changes to ongoing courses.


Numerous tools and techniques, which were time-consuming to implement, were ultimately not enabled in the experiments. The study would have been more successful if it had started earlier, allowing more time to accumulate students and participating courses. Furthermore, the courses that did participate had already been live for at least a few weeks, sometimes more than a month. It is likely that students have more questions that would’ve been appropriate for a chatbot at the beginning of a course. Therefore, the study would have also benefited from starting at the beginning of a study period, rather than in the middle of P4.


The feedback mechanism was designed to ask students questions at controlled time intervals. As detailed in \autoref{ch:methods}, this approach aimed to account for the students' familiarity with the AI assistant. The length of time a student used the tool could influence their responses. For this reason, the study was structured to introduce questions at specific intervals, which were determined after a certain number of chat sessions.


However, as can be seen in figures \ref{fig:usage_12_number_of_sessions_with_number_of_chats} and \ref{fig:usage_13_number_of_sessions_with_number_of_messages}, many users sent a substantial number of messages, but fewer users had more than two chat sessions. Therefore, a better measure of a user's familiarity with the assistant might have been the number of messages sent. This alternative method would have prompted more users to provide feedback at more consistent intervals.


\section{The tools, models and technologies that were selected for the experiments}


It's worth noting that the comparison between the open and closed-source models used in this experiment is not entirely fair. \textit{Mistral-7B-Instruct-v0.2} has 7 billion parameters. OpenAI discloses less about their models; however, \textit{GPT-3} has a confirmed 175 billion parameters. Less is known about \textit{GPT-4}, which is rumoured to have a \gls{SMoE} architecture, likely with at least as many, if not more, parameters than \textit{GPT-3}. Even though the number of parameters isn’t the best metric for determining intelligence in a model, it is one useful metric for comparing models. Had it been operationally and financially feasible to run a larger, more similar model, such as \textit{Mixtral-8x7B-Instruct-v0.1}, that would’ve made for a better comparison.


It could also be argued that the \textit{GPT-3} model could have been chosen, since that would’ve made for a fairer comparison. Since \textit{GPT-4} is the current best model, and one that likely would have been used implementing tools like this in real world applications, that model made sense to test with respect to measuring the general acceptance of AI assistants among students. Had there been more time, or more money to fund more expensive servers to run the larger open source models, the experiment would have been executed with different models. The same cannot be said for the embedding functions though.


Even though the embedding functions incorporated in the study were never tested for retrieval purposes, due to the number of participating courses, the models selected should be easier to compare. \textit{Mistral-7B-Instruct-v0.2} and \textit{GPT-4} score very differently on the available benchmarks. The embedding functions chosen, Salesforce’s \textit{SFR-Embedding-Mistral} and OpenAI’s \textit{text-embedding-3-large}, rank very similarly to each other on the \gls{MTEB}.


\section{Prompts and instructions aren't portable across models}




\section{Why AI assistants are helpful to students}


% Why students haven’t tried the assitant


\section{Ethical and privacy concerns}






% \section{Comparison to existing literature}


% Compare your findings with previous studies and discuss any similarities or differences.
%




% \sweExpl{Diskussion\\
% Förbättringsförslag?}
% \generalExpl{This can be a separate chapter or a section in the previous chapter.}


diskussion här


\cleardoublepage