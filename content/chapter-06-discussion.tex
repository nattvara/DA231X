\chapter{Discussion}
\label{ch:discussion}


This chapter will discuss the key findings from the research carried out in this thesis. It will evaluate the implications of the findings and compare it to existing literature. The discussion is structured to provide analysis of the results. It will also explore why some questions laid out in the introduction section have been left unanswered. Finally the chapter will include a brief discussion about the role of AI assistants in the broader context of AI in the educational setting.




\section{Feasibility of Open Source Technologies}


The study, and the software constructed to execute its experiments, show that it is feasible to build and deploy an AI assistant using open source technologies. The most popular models to implement AI assistants are commonly provided under proprietary licences and hidden behind pay-per-use APIs. Examples of this are the models built and sold by OpenAI.


The results laid out in \autoref{ch:whatYouDid} and \autoref{ch:resultsAndAnalysis} shows that an open source alternative is feasible from an implementation standpoint. The open source \gls{LLM} and embedding functions used in the study proved functional and provided comparable performance to proprietary solutions with respect to operational metrics such as response time and query generation time. One learning from operating the software constructed for the study is that the embedding function should probably be operated with a dedicated GPU to improve performance when computing embeddings of larger docs, such as during indexing.


\section{User Engagement and Satisfaction}


The study showed steady engagement from students across multiple different courses. The courses were all fairly different from each other. Some courses were self-study courses, without any lectures, some had regular lectures. Some courses had many participating students, some courses had as little as 30 students in the course.


There was a clear indication that the proprietary models such as OpenAI’s GPT-4 received more favourable feedback compared to its open source counterparts. Unfortunately, due to the volume of participating courses and the number of chats that were held, the decision was made to not test other variables, such as different \gls{RAG} techniques, or embedding functions.


\section{Why AI assistants are helpful to students}


Chapter \ref{ch:resultsAndAnalysis} included section~\ref{sec:qualitative_analysis_of_user_responses} which analysed the answers collected from students in three courses who had tried the chatbot. The responses to questions distributed in these courses provided valuable insights into why students responded like they did to the questions in section~\ref{sec:survey_questions_injected_into_the_chat}.


The first conclusion that can be drawn from the responses to the form is the benefit students feel of being able to quickly access information at any time. Instead of having to navigate extensive course materials, students can quickly get answers to their questions by simply asking the AI assistant in the course room. It is easy to forget that students are enrolled in multiple courses at the same time, each with a different course room utilising a different course room layout. Even if an individual course room is well-designed, the simple fact that another course room is poorly designed, can make navigation challenging.


Being able to ask an assistant anything, at any time, was particularly useful for logistical details such as deadlines, lecture schedules, and the links to course material. This was especially mentioned by the students in the \textit{MG2040 Assembly Technology} course, however, these themes were present in the answers from students in all three courses which distributed forms.


Another thing students appreciated was being able to use the chatbot to understand course content. For instance, in the \textit{LD1000} course, users noted that the AI assistant provided very extensive answers and good explanations.


One core component was the integration of citations. Citations aren’t necessarily required when constructing chatbot-applications. However, many students, across all courses, often mentioned the importance of understanding the source of the assistants responses for being able to trust the answer. The assistant’s ability to both provide comprehensive answers, and links to follow-up information, was cited as a key reason by some students, as why they had felt the bot was useful.


Some students even reported that they preferred asking the assistant some questions over asking the teachers and TAs. This was primarily due to the fact that the assistant was available all hours of the day, and could provide quicker responses. Also, some students reported that they felt their questions could be too simple or trivial for asking a teacher. This is an interesting insight because this suggests there are questions that students aren’t asking teachers today. Ensuring that students are comfortable asking questions about the course and its contents is key for ensuring that students learn efficiently.


\section{Methodological Improvements}


The software designed for the study aimed to test various tools and techniques commonly used when constructing AI assistants, in alignment with the goals outlined in \ref{sec:goals}. However, shortly after the study's launch, it became evident that the number of participating courses - and consequently, the number of participating students - was insufficient to provide statistically significant results for more than one feature.


The number of participating courses was a function of when the custom software, developed for the study, was finished. Several course administrators and teachers at KTH were contacted and showed interest in participating in the study. The most common reason for not participating was that the course did not run during the study period, i.e. the course responsible had courses in P1, P2 or P3, not P4. The second most common reason was that the teacher held courses that had already started, and did not want to make changes to ongoing courses.


Numerous tools and techniques, which were time-consuming to implement, were ultimately not enabled in the experiments. The study would have been more successful if it had started earlier, allowing more time to accumulate students and participating courses. Furthermore, the courses that did participate had already been live for at least a few weeks, sometimes more than a month. It is likely that students have more questions that would’ve been appropriate for a chatbot at the beginning of a course. Therefore, the study would have also benefited from starting at the beginning of a study period, rather than in the middle of P4.


The feedback mechanism was designed to ask students questions at controlled time intervals. As detailed in \autoref{ch:methods}, this approach aimed to account for the students' familiarity with the AI assistant. The length of time a student used the tool could influence their responses. For this reason, the study was structured to introduce questions at specific intervals, which were determined after a certain number of chat sessions.


However, as can be seen in figures \ref{fig:usage_12_number_of_sessions_with_number_of_chats} and \ref{fig:usage_13_number_of_sessions_with_number_of_messages}, many users sent a substantial number of messages, but fewer users had more than two chat sessions. Therefore, a better measure of a user's familiarity with the assistant might have been the number of messages sent. This alternative method would have prompted more users to provide feedback at more consistent intervals.


\section{The tools, models and technologies that were selected for the experiments}


It's worth noting that the comparison between the open and closed-source models used in this experiment is not entirely fair. \textit{Mistral-7B-Instruct-v0.2} has 7 billion parameters. OpenAI discloses less about their models; however, \textit{GPT-3} has a confirmed 175 billion parameters. Less is known about \textit{GPT-4}, which is rumoured to have a \gls{SMoE} architecture, likely with at least as many, if not more, parameters than \textit{GPT-3}. Even though the number of parameters isn’t the best metric for determining intelligence in a model, it is one useful metric for comparing models. Had it been operationally and financially feasible to run a larger, more similar model, such as \textit{Mixtral-8x7B-Instruct-v0.1}, that would’ve made for a better comparison.


It could also be argued that the \textit{GPT-3} model could have been chosen, since that would’ve made for a fairer comparison. Since \textit{GPT-4} is the current best model, and one that likely would have been used implementing tools like this in real world applications, that model made sense to test with respect to measuring the general acceptance of AI assistants among students. Had there been more time, or more money to fund more expensive servers to run the larger open source models, the experiment would have been executed with different models. The same cannot be said for the embedding functions though.


Even though the embedding functions incorporated in the study were never tested for retrieval purposes, due to the number of participating courses, the models selected should be easier to compare. \textit{Mistral-7B-Instruct-v0.2} and \textit{GPT-4} score very differently on the available benchmarks. The embedding functions chosen, Salesforce’s \textit{SFR-Embedding-Mistral} and OpenAI’s \textit{text-embedding-3-large}, rank very similarly to each other on the \gls{MTEB}.


\section{Prompts and Instructions Aren't Portable Across Models}


Just as the study was launched, Meta released the third iteration of their LLaMA family of models, LLaMA 3. This included \textit{LLaMA 3 8B}, an eight billion parameter model. A model of this size was feasible to operate and scored significantly higher than \textit{Mistral-7B-Instruct-v0.2} on most benchmarks. Since both are open source, \textit{LLaMA 3 8B} obviously made for a better candidate for testing open source \gls{LLM}.


The infrastructure to download and run the new model is very flexible and stable. The software was quickly adapted to switch the study from Mistral’s model to Meta’s latest model. However, it quickly became evident that the prompts used in the application were incompatible. As explained in section~\ref{sec:prompt_engineering}, prompts are a key component of getting the \gls{LLM} to perform as desired. In this study's software, this primarily involved hosting a chat with the user and some internal logic for constructing queries and indexing/retrieving documents. These prompts weren’t trivially ported to work well with LLaMA 3.


The prompts were designed and tested with \textit{Mistral-7B-Instruct-v0.2} and \textit{GPT-4} during the initial development of the study's software. Quickly adapting these prompts to also work with \textit{LLaMA 3 8B} wasn’t possible. One key takeaway for when constructing similar studies is that model selection must be done early and continuously monitored to ensure accurate test results. In this study, \textit{GPT-4} and \textit{Mistral-7B-Instruct-v0.2} used the same prompts. One could even argue that this should be a variable controlled for to ensure both models have the best available and most adapted instructions for them to use.




\section{Ethical and privacy concerns}


The deployment of an AI tool like the assistant developed for this raises several ethical concerns, primarily regarding privacy and the trust feel for AI tools. Some answers in the forms provided in this course show that many students hold great concerns about AI tools and assistants, and their role within their educational environment.


An important concern that was raised in responses from students was the potential monitoring of the questions and interactions with the chatbot by the teachers. As was raised by one student in the forms, they explicitly stated \textit{"[the chat] is monitored by the teacher"} as the reason they hadn’t used the chat bot. Even though the assistant did offer a way for teachers to view the anonymous chats students had with the assistant, students weren’t told this fact. It’s noteworthy that a student simply presumed this to be true.


When presenting the assistant to teachers interested in participating in the study it was raised as a requirement from all of them, that there had to be a way for them to view the conversations. For instance, if the chatbot had answered a question incorrectly, and a student referred to this answer, the teacher needed the ability to verify what the chatbot had said. Therefore, this seems like a pressing issue to work on before deploying chatbots, like the one in this course, more broadly across schools and universities.


Before using the assistant in this course there was a very clear and unambiguous ethics note. The note featured a section about anonymity. The user would remain completely anonymous when interacting with the chatbot. Even though this was the case some users still didn’t trust the AI assistant was anonymous.


In a classroom, with a teacher or TA, there is no chance for anonymity, at least without using anonymising tools, such as online polls. AI assistants, like the one constructed in this study, present a huge opportunity for allowing students to anonymously ask teachers questions. Remaining anonymous can be important for several reasons. Students can feel questions they ask might influence their grades or be too embarrassed to ask a question they think makes them seem stupid in front of their peers. Even though this bot, according to some users, failed to provide an environment of anonymity. This remains an important feature to explore further in future work.


At some point, it might be useful for the user to not remain anonymous. Students might want to forward their question to a teacher if they are suspicious the answer generated by the assistant might be inaccurate. It could also be the case that the bot will need personal information to respond to the students request.


The bot in this course only had access to information available to all students. Developing a version with access to student specific information, such as their past assignments or grades, puts increased scrutiny on the correctness of the systems. Allowing students to grant access to their data for specific queries might be an interesting avenue to explore. For instance, if a student asks questions like \textit{"how many points do I need on my next homework in order to meet the requirements for an A?"}, the chatbot could reply with an authorisation request from the student to view their past work. Mechanisms like this are key to explore, for improving student trust in their anonymity while enhancing the usefulness of the assistant.




% \section{Comparison to existing literature}


% Compare your findings with previous studies and discuss any similarities or differences.
%




% \sweExpl{Diskussion\\
% Förbättringsförslag?}
% \generalExpl{This can be a separate chapter or a section in the previous chapter.}


diskussion här


\cleardoublepage